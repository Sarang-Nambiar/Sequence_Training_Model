{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unique tags from the training data\n",
    "def get_unique_tags(sentence_tags):\n",
    "    all_tags = set()\n",
    "    for tags in sentence_tags:\n",
    "        for tag in tags:\n",
    "            all_tags.add(tag)\n",
    "    return all_tags\n",
    "\n",
    "# writing the final predictions into the dev folder for evaluation\n",
    "def write_output(path):\n",
    "    # Training:\n",
    "    sentences, sentence_tags = read_data(f'{path}/train')\n",
    "    all_tags = get_unique_tags(sentence_tags)      \n",
    "    perceptron = Perceptron(all_tags)\n",
    "    train(perceptron, sentences, sentence_tags, epochs=10)\n",
    "\n",
    "    # Test prediction:\n",
    "    with open(f'{path}/dev.in') as f:\n",
    "        test_data = f.read().splitlines()\n",
    "\n",
    "    test_sentences = extract_data_test(test_data)\n",
    "    with open(f'{path}/dev.p4.out', \"w\") as f:\n",
    "        for sentence in test_sentences:\n",
    "            predicted_labels = perceptron.predict(sentence)\n",
    "            for j in range(len(predicted_labels)):\n",
    "                f.write(sentence[j] + \" \" + predicted_labels[j] + \"\\n\")\n",
    "            else:\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "# writing the final predictions into the test folder for submission              \n",
    "def test_write_output(path, dataset_type):\n",
    "    # training\n",
    "    if dataset_type == \"es\":\n",
    "        file_path = 'Data/ES/train'\n",
    "    else:\n",
    "        file_path = 'Data/RU/train'\n",
    "    sentences, sentence_tags = read_data(file_path)\n",
    "    all_tags = get_unique_tags(sentence_tags) # getting the unique tags\n",
    "    perceptron = Perceptron(all_tags)\n",
    "    train(perceptron, sentences, sentence_tags, epochs=10)\n",
    "\n",
    "    # testing\n",
    "    with open(f'{path}/test.in') as f:\n",
    "        test_data = f.read().splitlines()\n",
    "\n",
    "    test_sentences = extract_data_test(test_data)\n",
    "    with open(f'{path}/test.p4.out', \"w\") as f:\n",
    "        for sentence in test_sentences:\n",
    "            predicted_labels = perceptron.predict(sentence)\n",
    "            for j in range(len(predicted_labels)):\n",
    "                f.write(sentence[j] + \" \" + predicted_labels[j] + \"\\n\")\n",
    "            else:\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "# To get the sentences and sentence tags from the file\n",
    "def read_data(file_path):\n",
    "    sentences = []\n",
    "    sentence_tags = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        sentence, tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                if line: \n",
    "                    token, label = line.split()\n",
    "                    sentence.append(token)\n",
    "                    tags.append(label)\n",
    "                else:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence_tags.append(tags)\n",
    "                    sentence, tags = [], []\n",
    "            except Exception:\n",
    "                # handling case for when RU tokens are . .. O and . ... O\n",
    "                token = line[:-1].strip()\n",
    "                label = line[-1]\n",
    "                sentence.append(token)\n",
    "                tags.append(label)\n",
    "    return sentences, sentence_tags\n",
    "\n",
    "def train(perceptron, sentences, sentence_tags, epochs=10000):\n",
    "    for _ in range(epochs):\n",
    "        for sentence, tags in zip(sentences, sentence_tags):\n",
    "            perceptron.update(sentence, tags)\n",
    "\n",
    "# extracting only sentences from the test data\n",
    "def extract_data_test(data):\n",
    "    L=[]\n",
    "    l = []\n",
    "    for sentence in data:\n",
    "        if sentence==\"\":\n",
    "            L.append(l)\n",
    "            l = []\n",
    "        else:\n",
    "            l.append(sentence)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, all_tags, learning_rate=0.001):\n",
    "        self.tags = list(all_tags)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = defaultdict(dict)\n",
    "        self.bias = defaultdict(float) \n",
    "    \n",
    "    def update(self, sentence, correct_tags):\n",
    "        pred_tags = self.predict(sentence)\n",
    "        \n",
    "        for token, correct_tag, pred_tag in zip(sentence, correct_tags, pred_tags):\n",
    "            if correct_tag != pred_tag:\n",
    "                self.weights[correct_tag][token] = self.weights[correct_tag].get(token, 0) + self.learning_rate\n",
    "                self.weights[pred_tag][token] = self.weights[pred_tag].get(token, 0) - self.learning_rate\n",
    "                self.bias[correct_tag] += self.learning_rate\n",
    "                self.bias[pred_tag] -= self.learning_rate\n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        prev_tag = \"START\"\n",
    "        pred_tags = []\n",
    "        for token in sentence:\n",
    "            scores = {tag: self.weights[tag].get(token, 0) + self.bias[tag] for tag in self.tags}\n",
    "            pred_tag = max(scores, key=scores.get)\n",
    "            pred_tags.append(pred_tag)\n",
    "            prev_tag = pred_tag\n",
    "        return pred_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting for ES dataset:\n",
    "write_output('Data/ES')\n",
    "# predicting for RU dataset:\n",
    "write_output('Data/RU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results:\n",
    "\n",
    "### **ES (Spanish) Results:**\n",
    "- **Entity Statistics:**\n",
    "  - Entities in gold data: `229`\n",
    "  - Entities in prediction: `411`\n",
    "  - Correct Entity count: `171`\n",
    "  - **Precision:** `0.4161`\n",
    "  - **Recall:** `0.7467`\n",
    "  - **F-score:** `0.5344`\n",
    "\n",
    "- **Sentiment Statistics:**\n",
    "  - Correct Sentiment count: `129`\n",
    "  - **Precision:** `0.3139`\n",
    "  - **Recall:** `0.5633`\n",
    "  - **F-score:** `0.4031`\n",
    "\n",
    "---\n",
    "\n",
    "### **RU (Russian) Results:**\n",
    "- **Entity Statistics:**\n",
    "  - Entities in gold data: `389`\n",
    "  - Entities in prediction: `420`\n",
    "  - Correct Entity count: `202`\n",
    "  - **Precision:** `0.4810`\n",
    "  - **Recall:** `0.5193`\n",
    "  - **F-score:** `0.4994`\n",
    "\n",
    "- **Sentiment Statistics:**\n",
    "  - Correct Sentiment count: `143`\n",
    "  - **Precision:** `0.3405`\n",
    "  - **Recall:** `0.3676`\n",
    "  - **F-score:** `0.3535`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the newly released test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing output for the newly released test set\n",
    "# ES\n",
    "test_write_output('Test/ES', 'es')\n",
    "# RU\n",
    "test_write_output('Test/RU', 'ru')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
