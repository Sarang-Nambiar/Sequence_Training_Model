{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class CRFSentimentAnalyzer:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = {}\n",
    "\n",
    "    def train(self, dataset):\n",
    "        for sentence, labels in dataset:\n",
    "            for i in range(len(sentence)):\n",
    "                features = self.extract_features(sentence, i)\n",
    "                predicted_label = self.predict(features, labels)\n",
    "                true_label = labels[i]\n",
    "                if predicted_label != true_label:\n",
    "                    self.update_weights(features, predicted_label, true_label)\n",
    "\n",
    "    def extract_features(self, sentence, i):\n",
    "        features = {}\n",
    "        word = sentence[i]\n",
    "        features[f'word:{word}'] = 1\n",
    "        # Add more feature extraction logic here if needed\n",
    "        for j in range(i - 1, i + 2):\n",
    "            if j >= 0 and j < len(sentence):\n",
    "                features[f'word-{j}:{sentence[j]}'] = 1\n",
    "        features[f'is_capitalized:{word[0].isupper()}'] = 1\n",
    "        features[f'is_punctuation:{re.match(r\"^[,.!?;]\", word) != None}'] = 1\n",
    "        return features\n",
    "\n",
    "    def predict(self, features, labels):\n",
    "        scores = {label: sum(features.get(feature, 0) * weight for feature, weight in self.weights.get(label, {}).items())\n",
    "                  for label in labels}\n",
    "        return max(scores, key=scores.get)\n",
    "\n",
    "    def update_weights(self, features, predicted_label, true_label):\n",
    "        if true_label not in self.weights:\n",
    "            self.weights[true_label] = {}\n",
    "        if predicted_label not in self.weights:\n",
    "            self.weights[predicted_label] = {}\n",
    "\n",
    "        for feature, value in features.items():\n",
    "            self.weights[true_label][feature] = self.weights[true_label].get(feature, 0) + self.learning_rate * value\n",
    "            self.weights[predicted_label][feature] = self.weights[predicted_label].get(feature, 0) - self.learning_rate * value\n",
    "\n",
    "    def classify(self, sentence):\n",
    "        predicted_labels = []\n",
    "        for i in range(len(sentence)):\n",
    "            features = self.extract_features(sentence, i)\n",
    "            predicted_label = self.predict(features, list(self.weights.keys()))\n",
    "            predicted_labels.append(predicted_label)\n",
    "        return predicted_labels\n",
    "\n",
    "\n",
    "def extract_data(data):\n",
    "    # data1 = data.split(\"\")\n",
    "    words = []\n",
    "    labels = []\n",
    "    l=[]\n",
    "    for sentence in data:\n",
    "        if sentence==\"\":\n",
    "            obj=(words,labels)\n",
    "            l.append((obj))\n",
    "            labels=[]\n",
    "            words=[]\n",
    "            obj=()\n",
    "        else:\n",
    "            a=sentence.split(\" \")\n",
    "\n",
    "            words.append(a[0])\n",
    "            labels.append(a[1])\n",
    "    return l\n",
    "\n",
    "def extract_data_test(data):\n",
    "    # data1 = data.split(\"\")\n",
    "\n",
    "    L=[]\n",
    "    l = []\n",
    "    for sentence in data:\n",
    "        if sentence==\"\":\n",
    "            L.append(l)\n",
    "            l = []\n",
    "        else:\n",
    "            l.append(sentence)\n",
    "    return L\n",
    "\n",
    "# Given dataset\n",
    "with open(r'Data\\ES\\train') as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "dataset = extract_data(data)\n",
    "\n",
    "# Initialize and train the sentiment analyzer\n",
    "analyzer = CRFSentimentAnalyzer()\n",
    "analyzer.train(dataset)\n",
    "\n",
    "# Test sentences\n",
    "with open(r'Data\\ES\\dev.in') as f:\n",
    "    test_data = f.read().splitlines()\n",
    "\n",
    "test_sentences = extract_data_test(test_data)\n",
    "\n",
    "# Perform sentiment analysis on test sentences\n",
    "with open(r'Data\\ES\\dev.p4.out', \"w+\") as f:\n",
    "    for sentence in test_sentences:\n",
    "        predicted_labels = analyzer.classify(sentence)\n",
    "        for j in range(len(predicted_labels)):\n",
    "            f.write(sentence[j] + \" \" + predicted_labels[j] + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.word_count = {}\n",
    "        self.class_count = {}\n",
    "        self.total_words = set()\n",
    "\n",
    "    def fit(self, data):\n",
    "        lines = data.split('\\n')\n",
    "        for line in lines:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "\n",
    "            word, tag = parts\n",
    "            self.total_words.add(word)\n",
    "            \n",
    "            if tag not in self.class_count:\n",
    "                self.class_count[tag] = 0\n",
    "            self.class_count[tag] += 1\n",
    "            \n",
    "            if tag not in self.word_count:\n",
    "                self.word_count[tag] = {}\n",
    "            if word not in self.word_count[tag]:\n",
    "                self.word_count[tag][word] = 0\n",
    "            self.word_count[tag][word] += 1\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        sentiments = []\n",
    "        for word in sentence.split():\n",
    "            max_prob = float('-inf')\n",
    "            best_tag = 'Outside'\n",
    "            for tag in self.class_count:\n",
    "                prob = self.class_count[tag]\n",
    "                # Use Laplace smoothing for word probabilities\n",
    "                word_prob = (self.word_count[tag].get(word, 0) + 1) / (self.class_count[tag] + len(self.total_words))\n",
    "                prob *= word_prob\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_tag = tag\n",
    "            sentiments.append(best_tag)\n",
    "        return sentiments\n",
    "\n",
    "# Example Usage:\n",
    "with open(r'Data\\\\ES\\\\train') as f:\n",
    "    data = f.read()\n",
    "\n",
    "classifier = NaiveBayes()\n",
    "classifier.fit(data)\n",
    "\n",
    "print(classifier.predict(\"Risotto\"))  # Expected: negative (based on training data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFSentimentAnalyzer:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = {}\n",
    "\n",
    "    def train(self, dataset):\n",
    "        for sentence, labels in dataset:\n",
    "            for i in range(len(sentence)):\n",
    "                features = self.extract_features(sentence, i)\n",
    "                predicted_label = self.predict(features, labels)\n",
    "                true_label = labels[i]\n",
    "                if predicted_label != true_label:\n",
    "                    self.update_weights(features, predicted_label, true_label)\n",
    "\n",
    "    def extract_features(self, sentence, i):\n",
    "        features = {}\n",
    "        word = sentence[i]\n",
    "        features[f'word:{word}'] = 1\n",
    "        # Add more feature extraction logic here if needed\n",
    "        return features\n",
    "\n",
    "    def predict(self, features, labels):\n",
    "        scores = {label: sum(features.get(feature, 0) * weight for feature, weight in self.weights.get(label, {}).items())\n",
    "                  for label in labels}\n",
    "        return max(scores, key=scores.get)\n",
    "\n",
    "    def update_weights(self, features, predicted_label, true_label):\n",
    "        if true_label not in self.weights:\n",
    "            self.weights[true_label] = {}\n",
    "        if predicted_label not in self.weights:\n",
    "            self.weights[predicted_label] = {}\n",
    "\n",
    "        for feature, value in features.items():\n",
    "            self.weights[true_label][feature] = self.weights[true_label].get(feature, 0) + self.learning_rate * value\n",
    "            self.weights[predicted_label][feature] = self.weights[predicted_label].get(feature, 0) - self.learning_rate * value\n",
    "\n",
    "    def classify(self, sentence):\n",
    "        predicted_labels = []\n",
    "        for i in range(len(sentence)):\n",
    "            features = self.extract_features(sentence, i)\n",
    "            predicted_label = self.predict(features, list(self.weights.keys()))\n",
    "            predicted_labels.append(predicted_label)\n",
    "        return predicted_labels\n",
    "\n",
    "# Given dataset\n",
    "# ...\n",
    "\n",
    "# Initialize and train the sentiment analyzer\n",
    "# ...\n",
    "\n",
    "# Test sentences\n",
    "# ...\n",
    "\n",
    "# Given dataset\n",
    "\n",
    "def extract_data(data):\n",
    "    # data1 = data.split(\"\")\n",
    "    words = []\n",
    "    labels = []\n",
    "    l=[]\n",
    "    for sentence in data:\n",
    "        if sentence==\"\":\n",
    "            obj=(words,labels)\n",
    "            l.append((obj))\n",
    "            labels=[]\n",
    "            words=[]\n",
    "            obj=()\n",
    "        else:\n",
    "            a=sentence.split(\" \")\n",
    "\n",
    "            words.append(a[0])\n",
    "            labels.append(a[1])\n",
    "    return l\n",
    "\n",
    "def extract_data_test(data):\n",
    "    # data1 = data.split(\"\")\n",
    "\n",
    "    L=[]\n",
    "    l = []\n",
    "    for sentence in data:\n",
    "        if sentence==\"\":\n",
    "            L.append(l)\n",
    "            l = []\n",
    "        else:\n",
    "            l.append(sentence)\n",
    "    return L\n",
    "\n",
    "with open(r'Data\\\\ES\\\\train') as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "\n",
    "dataset = extract_data(data)\n",
    "# for sentence in data:\n",
    "#     dataset.append(extract_data(sentence))\n",
    "\n",
    "\n",
    "# Initialize and train the sentiment analyzer\n",
    "analyzer = CRFSentimentAnalyzer()\n",
    "analyzer.train(dataset)\n",
    "\n",
    "# Test sentences\n",
    "with open (r'Data\\\\ES\\\\dev.in') as f:\n",
    "    test_data = f.read().splitlines()\n",
    "\n",
    "test_sentences = extract_data_test(test_data)\n",
    "\n",
    "# Perform sentiment analysis on test sentences\n",
    "with open(r'Data\\\\ES\\\\dev.p4.out', \"w+\") as f:\n",
    "    for sentence in test_sentences:\n",
    "        predicted_labels = analyzer.classify(sentence)\n",
    "        for j in range(len(predicted_labels)):\n",
    "            f.write(sentence[j] + \" \" + predicted_labels[j] + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
