{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "with open(\"Data/ES/train\") as f:\n",
    "    es = f.read().splitlines()\n",
    "\n",
    "with open(\"Data/RU/train\") as f:\n",
    "    ru = f.read().splitlines()\n",
    "\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Replace 'utf-8' with the appropriate encoding if needed\n",
    "# with open(\"Data/ES/train\", encoding=\"utf-8\") as f:\n",
    "#     es = f.read().splitlines()\n",
    "\n",
    "# # Replace 'utf-8' with the appropriate encoding if needed\n",
    "# with open(\"Data/RU/train\", encoding=\"utf-8\") as f:\n",
    "#     ru = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/ES/dev.in\") as f:\n",
    "    dev_in_es = f.read().splitlines()\n",
    "with open(\"Data/ES/dev.out\") as f:\n",
    "    dev_out_es = f.read().splitlines()\n",
    "with open(\"Data/RU/dev.in\") as f:\n",
    "    dev_in_ru = f.read().splitlines()\n",
    "with open(\"Data/RU/dev.out\") as f:\n",
    "    test_out_ru = f.read().splitlines()\n",
    "\n",
    "# with open(\"Data/ES/dev.in\", encoding=\"utf-8\") as f:\n",
    "#     dev_in_es = f.read().splitlines()\n",
    "# with open(\"Data/ES/dev.out\", encoding=\"utf-8\") as f:\n",
    "#     dev_out_es = f.read().splitlines()\n",
    "# with open(\"Data/RU/dev.in\", encoding=\"utf-8\") as f:\n",
    "#     dev_in_ru = f.read().splitlines()\n",
    "# with open(\"Data/RU/dev.out\",encoding=\"utf-8\") as f:\n",
    "#     test_out_ru = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_params(train_data,k=1):\n",
    "\n",
    "    word_sentiment_counts = defaultdict(lambda: defaultdict(int))\n",
    "    sentiment_counts =  defaultdict(int)\n",
    "    emission_params = {}\n",
    "    \n",
    "    #getting the count(y) and count(y --> x)\n",
    "    for sentence in train_data:\n",
    "        try:\n",
    "            if(sentence!=\"\"):\n",
    "                x, label = sentence.split(\" \")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        sentiment_counts[label]+=1\n",
    "\n",
    "        word_sentiment_counts[label][x]+=1\n",
    "\n",
    "\n",
    "\n",
    "    #calculating the emission parameters\n",
    "\n",
    "    for key in word_sentiment_counts:\n",
    "        for word in word_sentiment_counts[key]:\n",
    "            emission_params[(word,key)] = word_sentiment_counts[key][word]/(sentiment_counts[key])\n",
    "    # print(emission_params)\n",
    "    return emission_params, sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_para, count =estimate_emission_params(es)\n",
    "print(es_para[(\"palo\", \"O\")])\n",
    "states = []\n",
    "for i in count.keys():\n",
    "    states.append(i)\n",
    "print(\"count\", states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def estimate_transition_parameters_test(sentences):\n",
    "    transition_counts = {}\n",
    "    state_counts = {}\n",
    "    list_of_sentences = [list(sub) for ele, sub in groupby(sentences, key = bool) if ele]\n",
    "    for one_sentence in list_of_sentences:\n",
    "        # print(one_sentence)\n",
    "        prev_state = 'START'\n",
    "        for one_word in one_sentence:\n",
    "            if(one_word!=\"\"):\n",
    "                #print(one_word)\n",
    "                word, state = one_word.split(\" \")\n",
    "                if state_counts.get(prev_state):\n",
    "                    state_counts[prev_state] +=1\n",
    "                else:\n",
    "                    state_counts[prev_state] = 1\n",
    "                \n",
    "\n",
    "                if prev_state not in transition_counts:\n",
    "                    transition_counts[prev_state] = {}\n",
    "                if state not in transition_counts[prev_state]:\n",
    "                    transition_counts[prev_state][state] = 1\n",
    "                else:\n",
    "                    transition_counts[prev_state][state] += 1\n",
    "                prev_state = state\n",
    "        if \"END\" not in transition_counts[prev_state]:\n",
    "            transition_counts[prev_state][\"END\"] = 1\n",
    "        else: \n",
    "            transition_counts[prev_state][\"END\"] +=1\n",
    "        # print(transition_counts)\n",
    "    for from_state, to_states in transition_counts.items():\n",
    "        # print(f\"From State: {from_state}\")\n",
    "        for to_state, count in to_states.items():\n",
    "            transition_counts[from_state][to_state] = count/state_counts[from_state]\n",
    "            # print(f\"  To State: {to_state}, Count: {count}\")\n",
    "    # print(transition_counts)    \n",
    "\n",
    "            \n",
    "\n",
    "    # print(\"transition\", transition_counts, \"state\", state_counts)       \n",
    "    return transition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_transition_parameters_test(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def viterbi_algorithm_2(sentence, transition_params, emission_params, states):\n",
    "    n = len(sentence)\n",
    "    num_states = len(states)\n",
    "    viterbi = [{} for _ in range(n)]\n",
    "    backpointers = [{} for _ in range(n)]\n",
    "\n",
    "    # Initialization at time step 0\n",
    "    for state in states:\n",
    "        emission_prob = emission_params.get((sentence[0], state), 1e-10)\n",
    "        viterbi[0][state] = math.log(transition_params['START'].get(state, 1e-10)) + math.log(emission_prob)\n",
    "        backpointers[0][state] = 'START'\n",
    "\n",
    "    # Forward pass\n",
    "    for t in range(1, n):\n",
    "        for state in states:\n",
    "            max_prob = float('-inf')\n",
    "            prev_state = None\n",
    "            for prev_state in states:\n",
    "                transition_prob = transition_params[prev_state].get(state, 1e-10)\n",
    "                emission_prob = emission_params.get((sentence[t], state), 1e-10)\n",
    "                prob = viterbi[t - 1].get(prev_state,1e-10) + math.log(transition_prob) + math.log(emission_prob)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    backpointers[t][state] = prev_state\n",
    "            viterbi[t][state] = max_prob\n",
    "\n",
    "    # Termination step\n",
    "    max_prob = float('-inf')\n",
    "    final_state = None\n",
    "    for state in states:\n",
    "        # print(viterbi[n - 1][state])\n",
    "        transition_prob = transition_params[state].get('STOP', 1e-10)\n",
    "        prob = viterbi[n - 1][state] + math.log(transition_prob)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            final_state = state\n",
    "\n",
    "    # Backtracking step\n",
    "    best_path = [final_state]\n",
    "    for t in range(n - 1, 0, -1):\n",
    "        best_path.insert(0, backpointers[t][best_path[0]])\n",
    "\n",
    "    return best_path\n",
    "\n",
    "def run_viterbi_on_dev_set_2(dev_set, transition_params, emission_params, states):\n",
    "    output = []\n",
    "    list_of_sentences = [list(sub) for ele, sub in groupby(dev_set, key = bool) if ele]\n",
    "    for sentence in list_of_sentences:\n",
    "        best_path = viterbi_algorithm_2(sentence, transition_params, emission_params, states)\n",
    "        output.append(best_path)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_tags, predicted_tags):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for true_seq, pred_seq in zip(true_tags, predicted_tags):\n",
    "        for true, pred in zip(true_seq, pred_seq):\n",
    "            #print(true, pred, true == pred)\n",
    "            if true == pred and true != 'O':\n",
    "                tp += 1\n",
    "                #print(\"tp\", tp)\n",
    "            elif true != pred and true != 'O' and pred != 'O':\n",
    "                fp += 1\n",
    "                fn += 1  # Counting false negatives\n",
    "    #print(tp, fp, fn)      \n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Precision, Recall and F score for the given data\n",
    "def scores(labels, correct_labels):\n",
    "    correct = 0\n",
    "    total_predicted = 0\n",
    "    for true_seq, pred_seq in zip(correct_labels, labels):\n",
    "        for true, pred in zip(true_seq, pred_seq):\n",
    "            # print(true, pred, true == pred)\n",
    "            if true == pred:\n",
    "                correct += 1\n",
    "            total_predicted += 1\n",
    "    print(correct, total_predicted)\n",
    "    precision = correct / total_predicted\n",
    "    return precision\n",
    "\n",
    "\n",
    "def F_score(recall,precision):\n",
    "    v1=1/precision\n",
    "    v2=1/recall\n",
    "    f=2/(v1+v2)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume train_set, dev_in, and dev_out are lists of sentences and words with their corresponding tags\n",
    "\n",
    "# Train the model on the training set\n",
    "transition_params = estimate_transition_parameters_test(es)\n",
    "emission_params,count = estimate_emission_params(es)\n",
    "\n",
    "states = []\n",
    "for i in count.keys():\n",
    "    states.append(i)\n",
    "\n",
    "# Run Viterbi algorithm on the development set\n",
    "predicted_tags_2 = run_viterbi_on_dev_set_2(dev_in_es, transition_params, emission_params, states)\n",
    "\n",
    "def actual_tags(test_set):\n",
    "    tags =[]\n",
    "    list_of_sentences = [list(sub) for ele, sub in groupby(test_set, key = bool) if ele]\n",
    "    for sentence in list_of_sentences:\n",
    "        innerlist =[]\n",
    "        for word in sentence:\n",
    "            w, state = word.split()\n",
    "            innerlist.append(state)\n",
    "        tags.append(innerlist)\n",
    "    return tags\n",
    "\n",
    "# precision, recall, f_score = compute_metrics(actual_tags(dev_out_es), predicted_tags)\n",
    "precision_2 = scores(actual_tags(dev_out_es), predicted_tags_2)\n",
    "print(\"Precision2:\", precision_2)\n",
    "\n",
    "# Compute metrics\n",
    "# precision, recall, f_score = compute_metrics(dev_out_es, predicted_tags_2)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F-score:\", f_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "import math\n",
    "import sys\n",
    "import part1\n",
    "import os\n",
    "\n",
    "## This is code to obtain the transmission parameters\n",
    "\n",
    "# Outputs a dictionary {'label1': {'label1': count of label1->label1, 'label2': count of label1->label2...}...}\n",
    "def count_transmissions(data):\n",
    "    transmissions = {'START':{}, 'O':{},'B-positive':{},'B-neutral':{},'B-negative':{},'I-positive':{},'I-neutral':{},'I-negative':{}}\n",
    "    for label in transmissions.keys():\n",
    "        transmissions[label] = {'O':0,'B-positive':0,'B-neutral':0,'B-negative':0,'I-positive':0,'I-neutral':0,'I-negative':0, 'STOP':0}\n",
    "    last_position = 'START'\n",
    "    for line in data:\n",
    "        if line ==\"\\n\": #if line is a slash n - need to concern, rewrite processing script to include \\n\n",
    "            transmissions[last_position][\"STOP\"] += 1 #add transmission from last read label to STOP state\n",
    "            last_position = 'START' #reset state back to start\n",
    "        else:\n",
    "            transmissions[last_position][line[1]] += 1 #for transmission from the Label/State at LAST POSITION to the state read from the line\n",
    "            last_position = line[1] #move/transit to the next state read from the line\n",
    "    #transmissions[last_position]['STOP'] += 1 #add +1 for each transmission from label/state read to STOP\n",
    "    return transmissions\n",
    "\n",
    "# Outputs a dictionary {'label1': {'label1': probability of label1->label1, 'label2': probability of label1->label2...}...}\n",
    "def estimate_transmission_parameters(transmission_count, tags_count):\n",
    "    transmission_prob = {'START':{}, 'O':{},'B-positive':{},'B-neutral':{},'B-negative':{},'I-positive':{},'I-neutral':{},'I-negative':{},'STOP':{}}\n",
    "    for label in transmission_prob.keys():\n",
    "        transmission_prob[label] = {'START':0,'O':0,'B-positive':0,'B-neutral':0,'B-negative':0,'I-positive':0,'I-neutral':0,'I-negative':0, 'STOP':0}\n",
    "    for label_in, t_counts in transmission_count.items():\n",
    "        for label_out, count in t_counts.items():\n",
    "            transmission_prob[label_in][label_out] = count/tags_count[label_in]\n",
    "    \"\"\"\n",
    "    #Special Cases\n",
    "    for first_label, start_count in transmission_count['START'].items():\n",
    "        if start_count > 0:\n",
    "            transmission_prob['START'][first_label] = 1\n",
    "    \"\"\"\n",
    "    return transmission_prob\n",
    "\n",
    "# Viterbi algorithm to predict output labels on each document.\n",
    "# Note: should be called on EACH DOCUMENT of the VALIDATION/TEST set (data is a list of list containing strings)\n",
    "# a(u,v) is t_params\n",
    "# b(u,o) is e_params\n",
    "def viterbi(data, t_params, e_params, word_set):\n",
    "    # print(\"Beginning viterbi for\", data)\n",
    "    n = len(data)\n",
    "    # Includes only possible labels for the words in our dataset: ie. excludes 'START' and 'STOP'\n",
    "    labels = ['O', 'B-positive', 'B-neutral', 'B-negative', 'I-positive', 'I-neutral', 'I-negative', 'START']\n",
    "\n",
    "    # Initialization.\n",
    "    # cache is a list, where the list index represents the current position in the data\n",
    "    # each element consists of a dictionary of possible labels at that position\n",
    "    #IMPORTANT\n",
    "    # each possible label maps to a list of format [probability up to this point, parent label]\n",
    "    # 0: START 1: 1st word ... n: nth word n+1: END --> size = n+2\n",
    "\n",
    "    n_inf = -math.inf\n",
    "    cache = [{'START':[n_inf, None],\n",
    "    'STOP': [n_inf, None], \n",
    "    'O':[n_inf, None],\n",
    "    'B-positive':[n_inf, None],\n",
    "    'B-neutral':[n_inf, None],\n",
    "    'B-negative':[n_inf, None],\n",
    "    'I-positive':[n_inf, None],\n",
    "    'I-neutral':[n_inf, None],\n",
    "    'I-negative':[n_inf, None]} for i in range(n+2)]\n",
    "\n",
    "    cache[0]['START'][0] = 0 # Technically this should be 0\n",
    "\n",
    "    for j in range(0, n):\n",
    "        next_word = data[j]\n",
    "        # print(\"\\n\\n Step\", j+1, \"current word is:\", next_word)\n",
    "        # Iterate over all of the current labels in this step.\n",
    "        for u in labels:\n",
    "            # print(\"\\n Checking u: \", u)\n",
    "            maximum = n_inf\n",
    "            max_label = None\n",
    "            # Because we want to find the maximum v\n",
    "            for v in labels:\n",
    "                # If any of the observed probabilities are 0, we should skip because that is an impossible path\n",
    "                if (cache[j][v][0] == n_inf or t_params[v][u] == 0):\n",
    "                    # print(v, \"to\", u, \"is impossible\")\n",
    "                    continue\n",
    "                prev_cached_value = cache[j][v][0]\n",
    "                if next_word in word_set:\n",
    "                    if next_word not in e_params[u].keys():\n",
    "                        # print(\"impossible emission: word in training set yet not seen for this label.\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        emission_prob = e_params[u][next_word]\n",
    "                else:\n",
    "                    # print(\"not in training set. using the #UNK# probability\")\n",
    "                    emission_prob = e_params[u]['#UNK#']\n",
    "                transmission_prob = t_params[v][u]\n",
    "                # print(\"cache:\", prev_cached_value, \"emiss:\", emission_prob, \"trans:\", transmission_prob)\n",
    "                prob = prev_cached_value + math.log(emission_prob) + math.log(transmission_prob)\n",
    "                # print(v, 'to', u, 'emitting', next_word, 'has prob', prob)\n",
    "                if maximum < prob:\n",
    "                    maximum = prob\n",
    "                    max_label = v\n",
    "            \n",
    "            # print('best v is', max_label, 'with prob', maximum)\n",
    "            if maximum == n_inf:\n",
    "                continue\n",
    "            cache[j+1][u][0] = maximum\n",
    "            cache[j+1][u][1] = max_label\n",
    "    \n",
    "    # Final Step (n+1)\n",
    "    maximum = n_inf\n",
    "    max_label = None\n",
    "    for v in labels:\n",
    "        prev_cached_value = cache[n][v][0]\n",
    "        transmission_prob = t_params[v]['STOP']\n",
    "        if (prev_cached_value == 0 or transmission_prob == 0):\n",
    "            continue\n",
    "        prob = prev_cached_value + math.log(transmission_prob)\n",
    "        if maximum < prob:\n",
    "            maximum = prob\n",
    "            max_label = v\n",
    "\n",
    "    if maximum != n_inf:\n",
    "        cache[n+1]['STOP'][0] = maximum\n",
    "        cache[n+1]['STOP'][1] = max_label\n",
    "\n",
    "    # for i in range(len(cache)):\n",
    "    #     print(i, cache[i])\n",
    "    # print('\\n')\n",
    "    \n",
    "    # Finding the most probable labels.\n",
    "    output = ['' for i in range(n)]\n",
    "\n",
    "    # Default to \"O\" if emission isn't possible.\n",
    "    if max_label == None:\n",
    "        max_label = \"O\"\n",
    "\n",
    "    # for the n-1th to 1st word\n",
    "    for j in range(n+1, 1, -1):\n",
    "        # print(\"step\", j, \"old max:\", max_label, \"in cache:\", cache[j])\n",
    "        max_label = cache[j][max_label][1]\n",
    "        if max_label == None:\n",
    "            max_label = \"O\"\n",
    "        output[j-2] = max_label\n",
    "    \n",
    "    return output\n",
    "\n",
    "def viterbi_loop(separated, t_params, e_params, word_set):\n",
    "    final = []\n",
    "    for doc in separated:\n",
    "        final.append(viterbi(doc, t_params, e_params, word_set))\n",
    "    return final\n",
    "\n",
    "\n",
    "def run_viterbi(training_path, test_path, output_path):\n",
    "    train = utilities.read_data(training_path)\n",
    "    train_words = utilities.get_training_set_words(train)\n",
    "    test = utilities.read_dev(test_path)\n",
    "    tags = utilities.count_tags(train)\n",
    "    tag_words = utilities.count_tag_words(train)\n",
    "    transmission_counts = count_transmissions(train)\n",
    "    t_params = estimate_transmission_parameters(transmission_counts, tags)\n",
    "    e_params = part1.estimate_emission_parameters_with_unk(tags, tag_words)\n",
    "    prediction = viterbi_loop(test, t_params, e_params, train_words)\n",
    "    utilities.output_prediction(prediction, test, output_path)\n",
    "\n",
    "# # Testing viterbi\n",
    "# test = ['Con', 'lo', 'cual', 'en', 'el', 'comedor', 'tienes', 'que', 'levantar', 'mas', 'la', 'voz', \n",
    "# 'para', 'oirte', 'y', 'se', 'forma', 'un', 'ambiente', 'que', 'no', 'lo', 'que', 'se', 'espera', 'de', 'una', 'estrella', 'michelin', '.']\n",
    "# output_sequence = viterbi(test, t_params, e_params, train_words)\n",
    "# # print('\\n')\n",
    "# # print(\"ogiginal length\", len(test))\n",
    "# # print('\\n')\n",
    "# # print(test)\n",
    "# # print(\"output length\", len(output_sequence))\n",
    "# print('\\n')\n",
    "# print(output_sequence)\n",
    "\n",
    "## Actual viterbi calls\n",
    "if __name__ == '__main__':\n",
    "    n = len(sys.argv)\n",
    "\n",
    "    if n == 1:\n",
    "        run_viterbi(r\"ES/train\", r\"ES/dev.in\", r\"ES/dev.p2.out\")\n",
    "        run_viterbi(r\"RU/train\", r\"RU/dev.in\", r\"RU/dev.p2.out\")\n",
    "    else:\n",
    "        if n == 4:\n",
    "            run_viterbi(sys.argv[1], sys.argv[2], sys.argv[3])\n",
    "        else:\n",
    "            print(\"usage: python part2.py [train_path] [test_path] [output_path]\")\n",
    "    python_cmd = \"python3\"\n",
    "    if os.name != \"posix\":\n",
    "        python_cmd = \"python\"\n",
    "    #evaluation portion\n",
    "    print('The scores for the russian dataset is:')\n",
    "    os.system(f'{python_cmd} EvalScript/evalResult.py RU/dev.out RU/dev.p2.out')\n",
    "    print('The scores for the ES dataset is:')\n",
    "    os.system(f'{python_cmd} EvalScript/evalResult.py ES/dev.out ES/dev.p2.out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
